---
title: "multiExam"
author: "Eirikur Jonsson"
date: "09/06/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(st514)
library(tidyverse)
library(ggplot2)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(rcompanion)
library(cowplot)
library(psych)
library(boot)
library(caret)
source("https://imada.sdu.dk/~chdj/ST514/F20/BoxM.r")
data("T1-6")
df <- tbl
# Non-multiple-sclerosis (NMS) group data
nms <- subset(df, V6 == 0)
# Multiple-sclerosis (MS) group data
ms <- subset(df, V6 == 1)
```

Add Descriptive function here

# Data Preperation

## Testing for normality with qqplots Mahalanobis SD whole and half/ass

```{r}
x <- as.matrix(tbl[1:5])
x.mean <- colMeans(x)
S <- cov(x)
n <- nrow(x)
D2 <- rep(0, n)
for (i in 1:n) {
  D2[i] <- t(x[i,] - x.mean) %*% solve(S) %*% (x[i,] - x.mean)
}
#round(D2, 4)
#which(D2 < qchisq(0.5, 2))
p <- ncol(x)
D2 <- mahalanobis(x, colMeans(x), S)
result <- qqplot(qchisq(ppoints(n, a = 0.5), df = p), D2,
       ylab = "Mahalanobis distances",
       xlab = bquote("Quantiles of " ~ chi[.(p)]^2),
       main = bquote("Q-Q plot of Mahalanobis" * ~ D^2 * 
                       " vs. quantiles of" * ~ chi[.(p)]^2))
cor(result$x, result$y)
```


### Are the subsets means possible mean vectors in each.

Using Hotellings $T^2$

Here we test these hypotheses:

$$
h_0:\mu(MS) = \mu(NMS) \\
h_1: \mu(MS) \ne \mu(NMS)
$$

$$
h_0: \mu(NMS) = \mu(MS) \\
h_1: \mu(NMS) \ne \mu(MS)
$$

```{r}
msMatrix <- as.matrix(ms[1:5])
nmsMatrix <- as.matrix(nms[1:5])
msMeans <- colMeans(msMatrix)
nmsMeans <- colMeans(nmsMatrix)
msN <- nrow(msMatrix)
nmsN <- nrow(nmsMatrix)
msP <- ncol(msMatrix)
nmsP <- ncol(nmsMatrix)
msCov <- cov(msMatrix)
nmsCov <- cov(nmsMatrix)


# Start -  are ms mu possible mean vectors in nms
msInNms <- t(nmsMeans - msMeans)%*%solve(nmsCov)%*%(nmsMeans - msMeans)
msInNmsT2 <- round(nmsN * msInNms, 3)
msInNmsFval <- msInNmsT2 * (nmsN - nmsP)/((nmsN - 1)*nmsP)
msInNmsFvalP <- pf(msInNmsFval, nmsP, nmsN - nmsP, lower.tail = FALSE)
# Now we do the reverse
nmsInMs <- t(msMeans - nmsMeans)%*%solve(msCov)%*%(msMeans - nmsMeans)
nmsInMsT2 <- round(msN * nmsInMs, 3)
nmsInMsFval <- nmsInMsT2 * (msN - msP) / ((msN - 1) * msP)
nmsInMsFvalP <- pf(nmsInMsFval, msP, msN - msP, lower.tail = FALSE)

print(paste("Nms in MS ", msInNmsT2, "With p value ", msInNmsFvalP))
print(paste("MS in NMS ", nmsInMsT2, "with a p value ", nmsInMsFvalP))
```

From these we can see that we can reject the null hypotheses in both instances that the means from each subset is a possible mean vector in the other.

## Outliers
Using boxplots to see if there are outliers
```{r}
df <- tbl
colnames(df) <- c(paste("x", 1:5, sep=""), "d2")
dfz <- round(scale(df[1:5]), 1)
colnames(dfz) <- c(paste("z", 1:5, sep = ""))
outliers <- cbind(df[,1:5], 1:98, dfz, tbl[,5])
colnames(outliers)[c(6,12)] <- c("Obs", "d2")
max(outliers$d2)
which(outliers$d2 == max(outliers$d2))

#This function calculates the critical value
chi_crit_val <- qchisq(0.005, 5, lower.tail = F)
chi_crit_val

#This function prints out the outliers by observation number in relation to the treshold of 16.74
out <- which(outliers$d2 > chi_crit_val)
which(outliers$d2 > 16.74)
df <- df[-out,]
```

Function to standardize the values of each column

```{r}
outliers
```

```{r}
scree(df[1:5], pc = TRUE, factors = FALSE)
pca.model <- principal(df[1:5], nfactors = 2)
summary(pca.model)
pca.model$loadings
```
```{r}
pca <- prcomp(~ x1 + x2 + x3 + x4 + x5, data = df, scale = TRUE)

(loadings <- pca$rotation)

axes <- predict(pca, newdata = df)
print(head(axes, 4))
```
```{r}
dat <- cbind(df, axes)
dat$d2 <- as.factor(dat$d2)
train <- createDataPartition(dat$d2, p = 0.6, list = FALSE)
training <- dat[train,]
testing <- dat[-train,]

model_log <- train(d2~ PC1 + PC2, data = training, method = "glm", family = "binomial")
model_log

pred <- predict(model_log, newdata = testing)
caret::confusionMatrix(data = pred, testing$d2)
```

```{r}
pc2 <- ((df[3]*0.833) + (df[5] * 0.854))
pc1 <- ((df[1] * 0.756) + (df[2] * 0.798) + (df[4] * 0.777))
plot(pc1$x1, pc2$x3)
```

```{r}
df_log <- data.frame(pc1$x1, pc2$x3, df[6])
colnames(df_log) <- c("pc1", "pc2", "label")
df_log$label <- as.factor(df_log$label)
```

```{r}
train <- createDataPartition(df_log$label, p = 0.6, list = FALSE)
training <- df_log[train,]
testing <- df_log[-train,]

model_log <- train(label~., data = training, method = "glm", family = "binomial")
model_log

pred <- predict(model_log, newdata = testing)
caret::confusionMatrix(data = pred, testing$label)
```

```{r}
two.group.quadratic.classification <- function(data, grouping, newdata) {
  dat.split <- split(data, grouping)
  g1 <- as.data.frame(dat.split[1])
  g2 <- as.data.frame(dat.split[2])
  g1.means <- apply(g1, 2, mean)
  g2.means <- apply(g2, 2, mean)
  g1.covar <- cov(g1)
  g2.covar <- cov(g2)
  
  prediction <- apply(newdata, 1, function(y) {
    d2.y1 <- (y - g1.means) %*% solve(g1.covar) %*% (y - g1.means)
    d2.y2 <- (y - g2.means) %*% solve(g2.covar) %*% (y - g2.means)
    ifelse(d2.y1^2 > d2.y2^2, 2, 1)
  })
  
  class.table <- table(grouping, prediction, dnn = c('Actual Group','Predicted Group'))
  pred.errors <- sum(diag(t(apply(class.table, 2, rev)))) / dim(data)[1]
  results <- list('Prediction'=prediction, 'Table of Predictions'=class.table, 'Error Rate'=pred.errors)
  
  return(results)
}

tbl.perf <- two.group.quadratic.classification(df[,1:5], df[,6], df[,1:5])
tbl.perf
tbl.perf <- two.group.quadratic.classification(tbl[,2:4], tbl[,6], tbl[,2:4])
tbl.perf

tbl.perf <- two.group.quadratic.classification(tbl[,2:5], tbl[,6], tbl[,2:5])
tbl.perf
```
      
